{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "model = CatBoostRegressor(iterations=2000,\n",
    "                          learning_rate=3e-4,\n",
    "                          depth=10)\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "# Get predictions\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred=reg.predict(X_test)\n",
    "print(\"R2 Score\", (r2_score(y_test, preds)))\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "print(\"MSE\", (np.sqrt(mse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "# import imageio\n",
    "\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "\n",
    "class myDataset(Data.Dataset):\n",
    "    def __init__(self,X_train,y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X_train[idx], dtype = torch.float), torch.tensor(self.y_train[idx], dtype = torch.float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X_train.shape[0]\n",
    "\n",
    "my_data = myDataset(X_train, y_train)\n",
    "my_data_test = myDataset(X_test, y_test)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=my_data, \n",
    "    batch_size=128,\n",
    "    shuffle=False,)\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=my_data_test, \n",
    "    batch_size=128, \n",
    "    shuffle=False,)\n",
    "# this is one way to define a network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.hidden1 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.hidden3 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.hidden4 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.hidden5 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = F.relu(self.hidden4(x))\n",
    "        x = F.relu(self.hidden5(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n",
    "net = Net(n_feature=181, n_hidden=20, n_output=1)     # define the network\n",
    "# print(net)  # net architecture\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "def train(net, loader, optimizer, loss_func):\n",
    "    # train the network\n",
    "    losses = []\n",
    "    for t in loader:\n",
    "        x, y = t\n",
    "        prediction = net(x)     # input x and predict based on x\n",
    "        loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "        losses.append(loss.item())\n",
    "    print(sum(losses)/len(losses))\n",
    "    \n",
    "def test(net,loader):\n",
    "    pred = np.zeros((0,1))\n",
    "    y_true = np.zeros((0,1))\n",
    "    for i in loader:\n",
    "        x,y = i\n",
    "        opt = net(x)\n",
    "#         pred.append(opt.detach().numpy())\n",
    "#         y_true.append(y.detach().numpy())\n",
    "        pred = np.append(opt.detach().numpy(),pred)\n",
    "        y_true =np.append(y.detach().numpy(),y_true)\n",
    "    return pred,y_true\n",
    "\n",
    "    \n",
    "EPOCHS = 200\n",
    "    \n",
    "for i in range(EPOCHS):\n",
    "    train(net, loader, optimizer, loss_func)\n",
    "a,b = test(net, loader_test)\n",
    "    \n",
    "#     # plot and show learning process\n",
    "#     plt.cla()\n",
    "#     ax.set_title('Regression Analysis', fontsize=35)\n",
    "#     ax.set_xlabel('Independent variable', fontsize=24)\n",
    "#     ax.set_ylabel('Dependent variable', fontsize=24)\n",
    "#     ax.set_xlim(-1.05, 1.5)\n",
    "#     ax.set_ylim(-0.25, 1.25)\n",
    "#     ax.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n",
    "#     ax.plot(x.data.numpy(), prediction.data.numpy(), 'g-', lw=3)\n",
    "#     ax.text(1.0, 0.1, 'Step = %d' % t, fontdict={'size': 24, 'color':  'red'})\n",
    "#     ax.text(1.0, 0, 'Loss = %.4f' % loss.data.numpy(),\n",
    "#             fontdict={'size': 24, 'color':  'red'})\n",
    "\n",
    "#     # Used to return the plot as an image array \n",
    "#     # (https://ndres.me/post/matplotlib-animated-gifs-easily/)\n",
    "#     fig.canvas.draw()       # draw the canvas, cache the renderer\n",
    "#     image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "#     image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "#     my_images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def load_dataset(flatten=False):\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    print(y_train)\n",
    "    # normalize x\n",
    "    X_train = X_train.astype(float) / 255.\n",
    "    X_test = X_test.astype(float) / 255.\n",
    "    # we reserve the last 10000 training examples for validation\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "    if flatten:\n",
    "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "## Printing dimensions\n",
    "print(X_train.shape, y_train.shape)\n",
    "## Visualizing the first digit\n",
    "plt.imshow(X_train[0], cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing dimension of input images from N*28*28 to N*784\n",
    "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
    "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print('Train dimension:');print(X_train.shape)\n",
    "print('Test dimension:');print(X_test.shape)\n",
    "## Changing labels to one-hot encoded vector\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "print('Train labels dimension:');print(y_train.shape)\n",
    "print('Test labels dimension:');print(y_test.shape)\n",
    "y_train.shape\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "s = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining various initialization parameters for 784-512-256-10 MLP model\n",
    "# num_classes = y_train.shape[1]\n",
    "num_features = X_train.shape[1]\n",
    "num_output = y_train.shape[1]\n",
    "num_layers_0 = 10\n",
    "num_layers_1 = 10\n",
    "starter_learning_rate = 0.001\n",
    "regularizer_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for the input data\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "input_X = tf.placeholder('float32',shape =(None,num_features),name=\"input_X\")\n",
    "input_y = tf.placeholder('float32',shape = (None,1),name='input_Y')\n",
    "## for dropout layer\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)\n",
    "weights_0 = tf.Variable(tf.random_normal([num_features,num_layers_0], stddev=(1/tf.sqrt(float(num_features)))))\n",
    "bias_0 = tf.Variable(tf.random_normal([num_layers_0]))\n",
    "weights_1 = tf.Variable(tf.random_normal([num_layers_0,num_layers_1], stddev=(1/tf.sqrt(float(num_layers_0)))))\n",
    "bias_1 = tf.Variable(tf.random_normal([num_layers_1]))\n",
    "weights_2 = tf.Variable(tf.random_normal([num_layers_1,num_output], stddev=(1/tf.sqrt(float(num_layers_1)))))\n",
    "bias_2 = tf.Variable(tf.random_normal([num_output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing weigths and biases\n",
    "hidden_output_0 = tf.nn.relu(tf.matmul(input_X,weights_0)+bias_0)\n",
    "hidden_output_0_0 = tf.nn.dropout(hidden_output_0, keep_prob)\n",
    "hidden_output_1 = tf.nn.relu(tf.matmul(hidden_output_0_0,weights_1)+bias_1)\n",
    "hidden_output_1_1 = tf.nn.dropout(hidden_output_1, keep_prob)\n",
    "predicted_y = tf.matmul(hidden_output_1_1,weights_2) + bias_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the loss function\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_y,labels=input_y)) \\\n",
    "#         + regularizer_rate*(tf.reduce_sum(tf.square(bias_0)) + tf.reduce_sum(tf.square(bias_1)))\n",
    "\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(input_y, predicted_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variable learning rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, 0, 5, 0.85, staircase=True)\n",
    "## Adam optimzer for finding the right weight\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,var_list=[weights_0,weights_1,weights_2,\n",
    "                                                                         bias_0,bias_1,bias_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metrics definition\n",
    "correct_prediction = tf.equal(tf.argmax(y_train,1), tf.argmax(predicted_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training parameters\n",
    "batch_size = 128\n",
    "epochs=14\n",
    "dropout_prob = 0.6\n",
    "training_accuracy = []\n",
    "training_loss = []\n",
    "testing_accuracy = []\n",
    "s.run(tf.global_variables_initializer())\n",
    "for epoch in range(epochs):    \n",
    "    arr = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(arr)\n",
    "    for index in range(0,X_train.shape[0],batch_size):\n",
    "        s.run(optimizer, {input_X: X_train[arr[index:index+batch_size]],\n",
    "                          input_y: y_train[arr[index:index+batch_size]],\n",
    "                        keep_prob:dropout_prob})\n",
    "    training_accuracy.append(s.run(accuracy, feed_dict= {input_X:X_train, \n",
    "                                                         input_y: y_train,keep_prob:1}))\n",
    "    training_loss.append(s.run(loss, {input_X: X_train, \n",
    "                                      input_y: y_train,keep_prob:1}))\n",
    "    \n",
    "    ## Evaluation of model\n",
    "    testing_accuracy.append(accuracy_score(y_test.argmax(1), \n",
    "                            s.run(predicted_y, {input_X: X_test,keep_prob:1}).argmax(1)))\n",
    "    print(\"Epoch:{0}, Train loss: {1:.2f} Train acc: {2:.3f}, Test acc:{3:.3f}\".format(epoch,\n",
    "                                                                    training_loss[epoch],\n",
    "                                                                    training_accuracy[epoch],\n",
    "                                                                   testing_accuracy[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting chart of training and testing accuracy as a function of iterations\n",
    "iterations = list(range(epochs))\n",
    "plt.plot(iterations, training_accuracy, label='Train')\n",
    "plt.plot(iterations, testing_accuracy, label='Test')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('iterations')\n",
    "plt.show()\n",
    "print(\"Train Accuracy: {0:.2f}\".format(training_accuracy[-1]))\n",
    "print(\"Test Accuracy:{0:.2f}\".format(testing_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Import the electricity price and demand data\n",
    "df = dd.read_csv('data/Electricity/MRIMMeter/*')\n",
    "# Convert the dask dataset to a pandas dataframe\n",
    "df = df.compute().reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def area(row):\n",
    "#     if (row['ServiceProvider'] == 'CITIPOWER'):\n",
    "#         val = 'Inner Melbourne'\n",
    "#     elif (row['ServiceProvider'] == 'POWERCOR'):\n",
    "#         val = 'West Victoria'\n",
    "#     elif (row['ServiceProvider'] == 'TXU'):\n",
    "#         val = 'East & Northeast Victoria'\n",
    "#     elif (row['ServiceProvider'] == 'UNITED'):\n",
    "#         val = 'Southeast Victoria'\n",
    "        \n",
    "#     else:\n",
    "#         val = 'Northwest Victoria'\n",
    "        \n",
    "#     return val\n",
    "\n",
    "# merged['ProfileArea'] = merged.apply(area, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below code to export the final dataframe as csv\n",
    "df.to_csv('data/mrim_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROFILEAREA</th>\n",
       "      <th>SETTD</th>\n",
       "      <th>DCTC</th>\n",
       "      <th>DAILYT</th>\n",
       "      <th>VAL01</th>\n",
       "      <th>VAL02</th>\n",
       "      <th>VAL03</th>\n",
       "      <th>VAL04</th>\n",
       "      <th>VAL05</th>\n",
       "      <th>VAL06</th>\n",
       "      <th>...</th>\n",
       "      <th>VAL39</th>\n",
       "      <th>VAL40</th>\n",
       "      <th>VAL41</th>\n",
       "      <th>VAL42</th>\n",
       "      <th>VAL43</th>\n",
       "      <th>VAL44</th>\n",
       "      <th>VAL45</th>\n",
       "      <th>VAL46</th>\n",
       "      <th>VAL47</th>\n",
       "      <th>VAL48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CITIPOWER</td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>MRIM</td>\n",
       "      <td>4683409.892</td>\n",
       "      <td>112853.074</td>\n",
       "      <td>104363.755</td>\n",
       "      <td>97343.792</td>\n",
       "      <td>91889.809</td>\n",
       "      <td>87655.710</td>\n",
       "      <td>84326.314</td>\n",
       "      <td>...</td>\n",
       "      <td>98183.323</td>\n",
       "      <td>98045.639</td>\n",
       "      <td>98624.974</td>\n",
       "      <td>96991.198</td>\n",
       "      <td>93626.362</td>\n",
       "      <td>89399.151</td>\n",
       "      <td>89023.848</td>\n",
       "      <td>90937.854</td>\n",
       "      <td>93384.260</td>\n",
       "      <td>88587.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CITIPOWER</td>\n",
       "      <td>02/01/2016</td>\n",
       "      <td>MRIM</td>\n",
       "      <td>4144130.925</td>\n",
       "      <td>77929.991</td>\n",
       "      <td>71042.793</td>\n",
       "      <td>66741.556</td>\n",
       "      <td>64142.109</td>\n",
       "      <td>62648.539</td>\n",
       "      <td>61436.392</td>\n",
       "      <td>...</td>\n",
       "      <td>97929.623</td>\n",
       "      <td>96730.314</td>\n",
       "      <td>97220.276</td>\n",
       "      <td>95825.309</td>\n",
       "      <td>92681.934</td>\n",
       "      <td>88599.713</td>\n",
       "      <td>88396.739</td>\n",
       "      <td>90509.929</td>\n",
       "      <td>93627.695</td>\n",
       "      <td>88861.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CITIPOWER</td>\n",
       "      <td>03/01/2016</td>\n",
       "      <td>MRIM</td>\n",
       "      <td>4041606.415</td>\n",
       "      <td>77963.111</td>\n",
       "      <td>70789.426</td>\n",
       "      <td>65956.136</td>\n",
       "      <td>63012.869</td>\n",
       "      <td>61428.158</td>\n",
       "      <td>59990.272</td>\n",
       "      <td>...</td>\n",
       "      <td>97361.935</td>\n",
       "      <td>97577.840</td>\n",
       "      <td>98754.239</td>\n",
       "      <td>97170.341</td>\n",
       "      <td>92879.757</td>\n",
       "      <td>87043.133</td>\n",
       "      <td>85519.369</td>\n",
       "      <td>86854.749</td>\n",
       "      <td>89913.379</td>\n",
       "      <td>86008.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CITIPOWER</td>\n",
       "      <td>04/01/2016</td>\n",
       "      <td>MRIM</td>\n",
       "      <td>4616195.881</td>\n",
       "      <td>75279.507</td>\n",
       "      <td>68426.319</td>\n",
       "      <td>63795.872</td>\n",
       "      <td>61056.812</td>\n",
       "      <td>59522.298</td>\n",
       "      <td>58509.989</td>\n",
       "      <td>...</td>\n",
       "      <td>102678.321</td>\n",
       "      <td>101282.623</td>\n",
       "      <td>101068.884</td>\n",
       "      <td>98636.803</td>\n",
       "      <td>93252.262</td>\n",
       "      <td>86986.361</td>\n",
       "      <td>84871.066</td>\n",
       "      <td>86469.874</td>\n",
       "      <td>88806.064</td>\n",
       "      <td>84316.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CITIPOWER</td>\n",
       "      <td>05/01/2016</td>\n",
       "      <td>MRIM</td>\n",
       "      <td>4741002.102</td>\n",
       "      <td>73777.193</td>\n",
       "      <td>67108.140</td>\n",
       "      <td>62688.207</td>\n",
       "      <td>60262.346</td>\n",
       "      <td>58984.282</td>\n",
       "      <td>58108.533</td>\n",
       "      <td>...</td>\n",
       "      <td>107847.093</td>\n",
       "      <td>106699.428</td>\n",
       "      <td>107972.371</td>\n",
       "      <td>106113.647</td>\n",
       "      <td>100409.381</td>\n",
       "      <td>93305.393</td>\n",
       "      <td>90399.650</td>\n",
       "      <td>90884.729</td>\n",
       "      <td>93075.553</td>\n",
       "      <td>88424.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7755</th>\n",
       "      <td>VICAGL</td>\n",
       "      <td>27/03/2020</td>\n",
       "      <td>MRIM</td>\n",
       "      <td>3664284.471</td>\n",
       "      <td>67989.754</td>\n",
       "      <td>62345.774</td>\n",
       "      <td>58427.129</td>\n",
       "      <td>55754.684</td>\n",
       "      <td>53828.163</td>\n",
       "      <td>52736.694</td>\n",
       "      <td>...</td>\n",
       "      <td>112983.899</td>\n",
       "      <td>107637.483</td>\n",
       "      <td>102132.287</td>\n",
       "      <td>96617.361</td>\n",
       "      <td>90491.404</td>\n",
       "      <td>84066.124</td>\n",
       "      <td>77659.121</td>\n",
       "      <td>71903.767</td>\n",
       "      <td>80753.975</td>\n",
       "      <td>75458.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7756</th>\n",
       "      <td>VICAGL</td>\n",
       "      <td>28/03/2020</td>\n",
       "      <td>MRIM</td>\n",
       "      <td>3550058.705</td>\n",
       "      <td>68166.934</td>\n",
       "      <td>62361.536</td>\n",
       "      <td>58282.243</td>\n",
       "      <td>55563.968</td>\n",
       "      <td>53556.083</td>\n",
       "      <td>51940.222</td>\n",
       "      <td>...</td>\n",
       "      <td>117317.812</td>\n",
       "      <td>111252.409</td>\n",
       "      <td>105175.338</td>\n",
       "      <td>99322.338</td>\n",
       "      <td>93706.852</td>\n",
       "      <td>87625.480</td>\n",
       "      <td>81024.166</td>\n",
       "      <td>75160.148</td>\n",
       "      <td>83452.236</td>\n",
       "      <td>78270.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7757</th>\n",
       "      <td>VICAGL</td>\n",
       "      <td>29/03/2020</td>\n",
       "      <td>MRIM</td>\n",
       "      <td>3768120.264</td>\n",
       "      <td>70987.767</td>\n",
       "      <td>64965.571</td>\n",
       "      <td>60702.308</td>\n",
       "      <td>57468.594</td>\n",
       "      <td>55047.343</td>\n",
       "      <td>53473.954</td>\n",
       "      <td>...</td>\n",
       "      <td>109857.531</td>\n",
       "      <td>105285.624</td>\n",
       "      <td>100358.961</td>\n",
       "      <td>94778.876</td>\n",
       "      <td>88539.298</td>\n",
       "      <td>81712.953</td>\n",
       "      <td>74875.006</td>\n",
       "      <td>68942.722</td>\n",
       "      <td>78092.730</td>\n",
       "      <td>73797.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7758</th>\n",
       "      <td>VICAGL</td>\n",
       "      <td>30/03/2020</td>\n",
       "      <td>MRIM</td>\n",
       "      <td>3630628.975</td>\n",
       "      <td>67004.949</td>\n",
       "      <td>61331.331</td>\n",
       "      <td>57437.539</td>\n",
       "      <td>54699.460</td>\n",
       "      <td>52482.356</td>\n",
       "      <td>51285.551</td>\n",
       "      <td>...</td>\n",
       "      <td>109994.933</td>\n",
       "      <td>104739.457</td>\n",
       "      <td>99656.454</td>\n",
       "      <td>94486.382</td>\n",
       "      <td>87854.683</td>\n",
       "      <td>81217.427</td>\n",
       "      <td>74501.693</td>\n",
       "      <td>68247.746</td>\n",
       "      <td>77265.842</td>\n",
       "      <td>72810.983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7759</th>\n",
       "      <td>VICAGL</td>\n",
       "      <td>31/03/2020</td>\n",
       "      <td>MRIM</td>\n",
       "      <td>3564775.325</td>\n",
       "      <td>66008.816</td>\n",
       "      <td>60593.087</td>\n",
       "      <td>56914.023</td>\n",
       "      <td>54288.741</td>\n",
       "      <td>52459.651</td>\n",
       "      <td>51334.667</td>\n",
       "      <td>...</td>\n",
       "      <td>110595.515</td>\n",
       "      <td>105215.091</td>\n",
       "      <td>100182.239</td>\n",
       "      <td>95100.902</td>\n",
       "      <td>88076.551</td>\n",
       "      <td>81126.662</td>\n",
       "      <td>74584.345</td>\n",
       "      <td>68594.546</td>\n",
       "      <td>77783.911</td>\n",
       "      <td>73151.636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7760 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PROFILEAREA       SETTD  DCTC       DAILYT       VAL01       VAL02  \\\n",
       "0      CITIPOWER  01/01/2016  MRIM  4683409.892  112853.074  104363.755   \n",
       "1      CITIPOWER  02/01/2016  MRIM  4144130.925   77929.991   71042.793   \n",
       "2      CITIPOWER  03/01/2016  MRIM  4041606.415   77963.111   70789.426   \n",
       "3      CITIPOWER  04/01/2016  MRIM  4616195.881   75279.507   68426.319   \n",
       "4      CITIPOWER  05/01/2016  MRIM  4741002.102   73777.193   67108.140   \n",
       "...          ...         ...   ...          ...         ...         ...   \n",
       "7755      VICAGL  27/03/2020  MRIM  3664284.471   67989.754   62345.774   \n",
       "7756      VICAGL  28/03/2020  MRIM  3550058.705   68166.934   62361.536   \n",
       "7757      VICAGL  29/03/2020  MRIM  3768120.264   70987.767   64965.571   \n",
       "7758      VICAGL  30/03/2020  MRIM  3630628.975   67004.949   61331.331   \n",
       "7759      VICAGL  31/03/2020  MRIM  3564775.325   66008.816   60593.087   \n",
       "\n",
       "          VAL03      VAL04      VAL05      VAL06  ...       VAL39       VAL40  \\\n",
       "0     97343.792  91889.809  87655.710  84326.314  ...   98183.323   98045.639   \n",
       "1     66741.556  64142.109  62648.539  61436.392  ...   97929.623   96730.314   \n",
       "2     65956.136  63012.869  61428.158  59990.272  ...   97361.935   97577.840   \n",
       "3     63795.872  61056.812  59522.298  58509.989  ...  102678.321  101282.623   \n",
       "4     62688.207  60262.346  58984.282  58108.533  ...  107847.093  106699.428   \n",
       "...         ...        ...        ...        ...  ...         ...         ...   \n",
       "7755  58427.129  55754.684  53828.163  52736.694  ...  112983.899  107637.483   \n",
       "7756  58282.243  55563.968  53556.083  51940.222  ...  117317.812  111252.409   \n",
       "7757  60702.308  57468.594  55047.343  53473.954  ...  109857.531  105285.624   \n",
       "7758  57437.539  54699.460  52482.356  51285.551  ...  109994.933  104739.457   \n",
       "7759  56914.023  54288.741  52459.651  51334.667  ...  110595.515  105215.091   \n",
       "\n",
       "           VAL41       VAL42       VAL43      VAL44      VAL45      VAL46  \\\n",
       "0      98624.974   96991.198   93626.362  89399.151  89023.848  90937.854   \n",
       "1      97220.276   95825.309   92681.934  88599.713  88396.739  90509.929   \n",
       "2      98754.239   97170.341   92879.757  87043.133  85519.369  86854.749   \n",
       "3     101068.884   98636.803   93252.262  86986.361  84871.066  86469.874   \n",
       "4     107972.371  106113.647  100409.381  93305.393  90399.650  90884.729   \n",
       "...          ...         ...         ...        ...        ...        ...   \n",
       "7755  102132.287   96617.361   90491.404  84066.124  77659.121  71903.767   \n",
       "7756  105175.338   99322.338   93706.852  87625.480  81024.166  75160.148   \n",
       "7757  100358.961   94778.876   88539.298  81712.953  74875.006  68942.722   \n",
       "7758   99656.454   94486.382   87854.683  81217.427  74501.693  68247.746   \n",
       "7759  100182.239   95100.902   88076.551  81126.662  74584.345  68594.546   \n",
       "\n",
       "          VAL47      VAL48  \n",
       "0     93384.260  88587.762  \n",
       "1     93627.695  88861.021  \n",
       "2     89913.379  86008.214  \n",
       "3     88806.064  84316.744  \n",
       "4     93075.553  88424.217  \n",
       "...         ...        ...  \n",
       "7755  80753.975  75458.512  \n",
       "7756  83452.236  78270.962  \n",
       "7757  78092.730  73797.197  \n",
       "7758  77265.842  72810.983  \n",
       "7759  77783.911  73151.636  \n",
       "\n",
       "[7760 rows x 52 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
